{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# for reproducibility\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Convolution2D, MaxPooling2D,ZeroPadding2D,AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.initializers import he_normal \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from keras.applications.inception_v3 import InceptionV3\n",
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path):\n",
    "    x_train_list=[]\n",
    "    y_train_list=[]\n",
    "    \n",
    "    data_dir = os.walk(train_path)\n",
    "    for root, dirs, files in data_dir:\n",
    "        if dirs==[] and files!=[]:\n",
    "            for f in files:\n",
    "                x_train_list.append(os.path.join(root, f))\n",
    "                y_train_list.append(root.split(\"/\")[-1])\n",
    "                \n",
    "    return x_train_list, y_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"datasets/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = {'abnormal':0, 'normal':1}\n",
    "\n",
    "x_train_list, y_train_list = load_data(train_path)\n",
    "        \n",
    "s = pd.DataFrame(y_train_list,columns = [\"name\"])\n",
    "s[\"name\"] = s[\"name\"].map(classes_list)\n",
    "\n",
    "y_train = np.array(s[\"name\"])\n",
    "\n",
    "\n",
    "x_train = []\n",
    "for i,each in enumerate(x_train_list):\n",
    "    try:\n",
    "        img = cv2.imread(each)\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = preprocess_input(img)\n",
    "        x_train.append(img)\n",
    "    except:\n",
    "        y_train = np.delete(y_train,i)\n",
    "        continue\n",
    "    \n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f329033be80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(y_train)\n",
    "plt.imshow(x_train[888], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cfc41cec193b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2054\u001b[0m                      random_state=random_state)\n\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \"\"\"\n\u001b[0;32m-> 1624\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.05, stratify = y_train)\n",
    "\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.eye(2)[y_train]\n",
    "Y_valid = np.eye(2)[y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 5\n",
    "nb_epoch = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 224, 224, 3) (883, 2) (47, 224, 224, 3) (47, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (883, 224, 224, 3)\n",
      "883 train samples\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "for i in range(3):\n",
    "    X_train[:,:,:,i]= X_train[:,:,:,i]\n",
    "\n",
    "    X_valid[:,:,:,i]= X_valid[:,:,:,i]\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        fill_mode = \"constant\",\n",
    "        zoom_range = 1,\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50( weights='imagenet', include_top=False)\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(256, activation='relu')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = Dense(512, activation='relu')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.4)(x)\n",
    "\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#for i,layer in enumerate(base_model.layers):\n",
    "#    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(lr=10e-6), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.fit(X_train,Y_train,epochs=25,batch_size=batch_size)\n",
    "#datagen.flow(X_train,Y_train,batch_size=batch_size),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "callbacks=callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., verbose=1, validation_data=(array([[[..., steps_per_epoch=27, epochs=50)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 [==============================] - 17s 627ms/step - loss: 0.5401 - acc: 0.7356 - val_loss: 0.2057 - val_acc: 0.9362\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 7s 253ms/step - loss: 0.2703 - acc: 0.9097 - val_loss: 0.0898 - val_acc: 0.9787\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 7s 247ms/step - loss: 0.1758 - acc: 0.9467 - val_loss: 0.0634 - val_acc: 0.9787\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.1387 - acc: 0.9629 - val_loss: 0.0451 - val_acc: 0.9787\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.1119 - acc: 0.9583 - val_loss: 0.0355 - val_acc: 0.9787\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0954 - acc: 0.9680 - val_loss: 0.0292 - val_acc: 0.9787\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0828 - acc: 0.9691 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0651 - acc: 0.9745 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0648 - acc: 0.9711 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0761 - acc: 0.9714 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0574 - acc: 0.9815 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0507 - acc: 0.9815 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 7s 245ms/step - loss: 0.0584 - acc: 0.9749 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0361 - acc: 0.9896 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0542 - acc: 0.9815 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0459 - acc: 0.9861 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0361 - acc: 0.9849 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 7s 246ms/step - loss: 0.0269 - acc: 0.9930 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0347 - acc: 0.9873 - val_loss: 8.6462e-04 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 7s 246ms/step - loss: 0.0428 - acc: 0.9873 - val_loss: 7.1354e-04 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0275 - acc: 0.9907 - val_loss: 6.8192e-04 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0443 - acc: 0.9838 - val_loss: 7.7795e-04 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0429 - acc: 0.9819 - val_loss: 7.1161e-04 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0488 - acc: 0.9849 - val_loss: 5.5527e-04 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 7s 247ms/step - loss: 0.0324 - acc: 0.9849 - val_loss: 9.4840e-04 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0331 - acc: 0.9877 - val_loss: 5.6842e-04 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0296 - acc: 0.9896 - val_loss: 6.7726e-04 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0438 - acc: 0.9849 - val_loss: 8.8011e-04 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0243 - acc: 0.9931 - val_loss: 7.8525e-04 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0350 - acc: 0.9873 - val_loss: 3.6957e-04 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0334 - acc: 0.9896 - val_loss: 4.1844e-04 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0346 - acc: 0.9869 - val_loss: 3.8774e-04 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 7s 253ms/step - loss: 0.0353 - acc: 0.9861 - val_loss: 6.4292e-04 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0232 - acc: 0.9907 - val_loss: 7.2926e-04 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0237 - acc: 0.9919 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0340 - acc: 0.9873 - val_loss: 5.3161e-04 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 7s 243ms/step - loss: 0.0191 - acc: 0.9942 - val_loss: 4.9048e-04 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0234 - acc: 0.9896 - val_loss: 4.1960e-04 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0171 - acc: 0.9954 - val_loss: 5.5308e-04 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 7s 247ms/step - loss: 0.0231 - acc: 0.9896 - val_loss: 5.4806e-04 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0319 - acc: 0.9873 - val_loss: 4.5561e-04 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 7s 249ms/step - loss: 0.0151 - acc: 0.9942 - val_loss: 4.7019e-04 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 7s 250ms/step - loss: 0.0147 - acc: 0.9965 - val_loss: 4.4511e-04 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 7s 245ms/step - loss: 0.0225 - acc: 0.9907 - val_loss: 5.5034e-04 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 7s 248ms/step - loss: 0.0231 - acc: 0.9946 - val_loss: 5.4190e-04 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0302 - acc: 0.9884 - val_loss: 7.4167e-04 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 7s 252ms/step - loss: 0.0202 - acc: 0.9907 - val_loss: 7.6550e-04 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 7s 245ms/step - loss: 0.0332 - acc: 0.9873 - val_loss: 5.9740e-04 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 7s 254ms/step - loss: 0.0165 - acc: 0.9942 - val_loss: 5.9463e-04 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 7s 251ms/step - loss: 0.0167 - acc: 0.9931 - val_loss: 0.0017 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train,Y_train,batch_size=batch_size),\n",
    "                            samples_per_epoch=X_train.shape[0],nb_epoch=nb_epoch,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_name='model_weight_with_ResNet50.h5'\n",
    "model.save(weight_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object walk at 0x7fd4a4d29db0>\n",
      "datasets/test/abnormal\n",
      "[]\n",
      "496\n",
      "<generator object walk at 0x7fd4a4d29db0>\n",
      "datasets/test/normal\n",
      "[]\n",
      "229\n",
      "2.71088592463 0.529737206086\n",
      "predict    0    1\n",
      "label            \n",
      "0        157  338\n",
      "1          2  226\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.32      0.48       495\n",
      "          1       0.40      0.99      0.57       228\n",
      "\n",
      "avg / total       0.80      0.53      0.51       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"datasets/test\"\n",
    "\n",
    "x_test_list, y_test_list = load_data(test_path)\n",
    "        \n",
    "s = pd.DataFrame(y_test_list,columns = [\"name\"])\n",
    "s[\"name\"] = s[\"name\"].map(classes_list)\n",
    "\n",
    "y_test = np.array(s[\"name\"])\n",
    "\n",
    "\n",
    "X_test = []\n",
    "for i,each in enumerate(x_test_list):\n",
    "    try:\n",
    "        img = cv2.imread(each)\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = preprocess_input(img)\n",
    "        X_test.append(img)\n",
    "    except:\n",
    "        y_test = np.delete(y_test,i)\n",
    "        continue\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.eye(2)[y_test]\n",
    "result=model.predict(X_test, verbose=0, steps=None)\n",
    "scores = model.evaluate(X_test[:,:,:,:], Y_test[:,:], verbose=0)\n",
    "print(scores[0],scores[1])\n",
    "print(pd.crosstab(np.array(y_test), np.array(np.argmax(result, axis=1)),\n",
    "                      rownames=['label'], colnames=['predict']))\n",
    "print(classification_report(np.array(y_test), np.array(np.argmax(result, axis=1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18623457192 0.94\n",
      "predict   0   1\n",
      "label          \n",
      "0        50   0\n",
      "1         6  44\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        50\n",
      "          1       1.00      0.88      0.94        50\n",
      "\n",
      "avg / total       0.95      0.94      0.94       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"datasets/test2\"\n",
    "\n",
    "x_test_list, y_test_list = load_data(test_path)\n",
    "        \n",
    "s = pd.DataFrame(y_test_list,columns = [\"name\"])\n",
    "s[\"name\"] = s[\"name\"].map(classes_list)\n",
    "\n",
    "y_test = np.array(s[\"name\"])\n",
    "\n",
    "X_test = []\n",
    "for i,each in enumerate(x_test_list):\n",
    "    try:\n",
    "        img = cv2.imread(each)\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = preprocess_input(img)\n",
    "        X_test.append(img)\n",
    "    except:\n",
    "        y_test = np.delete(y_test,i)\n",
    "        continue\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.eye(2)[y_test]\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "result=model.predict(X_test, verbose=0, steps=None)\n",
    "print(scores[0],scores[1])\n",
    "print(pd.crosstab(np.array(y_test), np.array(np.argmax(result, axis=1)),\n",
    "                      rownames=['label'], colnames=['predict']))\n",
    "print(classification_report(np.array(y_test), np.array(np.argmax(result, axis=1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
